{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison Notebook\n",
    "\n",
    "This notebook compares the performance of different models (Linear, XGBoost, and ResNet) in predicting the top 3 dogs for given human images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.doggelganger.utils import get_embedding, load_model as load_embedding_model\n",
    "from src.doggelganger.models import LinearRegressionModel, XGBoostModel, ResNetModel\n",
    "from src.doggelganger.train import make_training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embedding model\n",
    "embedding_model = load_embedding_model()\n",
    "\n",
    "# Load the training data\n",
    "X, y = make_training_data(\"../data/train\")\n",
    "\n",
    "# Load the test data (assuming you have a separate test set)\n",
    "X_test, y_test = make_training_data(\"../data/test\")\n",
    "\n",
    "# Load the trained models\n",
    "linear_model = LinearRegressionModel.load(\"../weights/alignment_model_linear.pt\")\n",
    "xgb_model = XGBoostModel.load(\"../weights/alignment_model_xgboost.pt\")\n",
    "resnet_model = ResNetModel.load(\"../weights/alignment_model_resnet.pt\", embedding_dim=X.shape[1])\n",
    "\n",
    "models = {\n",
    "    \"Linear\": linear_model,\n",
    "    \"XGBoost\": xgb_model,\n",
    "    \"ResNet\": resnet_model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_dogs(human_embedding, animal_embeddings, k=3):\n",
    "    similarities = cosine_similarity(human_embedding.reshape(1, -1), animal_embeddings)[0]\n",
    "    top_k_indices = np.argsort(similarities)[-k:][::-1]\n",
    "    return top_k_indices\n",
    "\n",
    "def plot_images(images, titles, main_title):\n",
    "    fig, axes = plt.subplots(1, len(images), figsize=(15, 5))\n",
    "    fig.suptitle(main_title, fontsize=16)\n",
    "    \n",
    "    for i, (img, title) in enumerate(zip(images, titles)):\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(title)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_top_3_dogs(human_image_path, models, animal_embeddings):\n",
    "    human_embedding = get_embedding(human_image_path, embedding_model)\n",
    "    \n",
    "    human_image = Image.open(human_image_path)\n",
    "    \n",
    "    plot_images([human_image], [\"Human\"], \"Input Human Image\")\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        predicted_embedding = model.predict(human_embedding.reshape(1, -1))[0]\n",
    "        top_3_indices = get_top_k_dogs(predicted_embedding, animal_embeddings)\n",
    "        \n",
    "        top_3_images = []\n",
    "        top_3_titles = []\n",
    "        \n",
    "        for i, idx in enumerate(top_3_indices, 1):\n",
    "            animal_image_path = f\"../data/train/animal/{idx:04d}.jpg\"\n",
    "            animal_image = Image.open(animal_image_path)\n",
    "            top_3_images.append(animal_image)\n",
    "            top_3_titles.append(f\"Top {i}\")\n",
    "        \n",
    "        plot_images(top_3_images, top_3_titles, f\"{model_name} Model - Top 3 Dogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top 3 dogs for a few sample human images\n",
    "sample_human_images = [\n",
    "    \"../data/test/human/0001.jpg\",\n",
    "    \"../data/test/human/0002.jpg\",\n",
    "    \"../data/test/human/0003.jpg\"\n",
    "]\n",
    "\n",
    "for human_image_path in sample_human_images:\n",
    "    visualize_top_3_dogs(human_image_path, models, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Comparison\n",
    "\n",
    "Let's compare the performance of the different models using various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from src.doggelganger.train import calculate_accuracies\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    predictions = model.predict(X)\n",
    "    mse = mean_squared_error(y, predictions)\n",
    "    r2 = r2_score(y, predictions)\n",
    "    top1_acc, top3_acc, top10_acc = calculate_accuracies(y, predictions)\n",
    "    \n",
    "    return {\n",
    "        \"MSE\": mse,\n",
    "        \"R2\": r2,\n",
    "        \"Top-1 Accuracy\": top1_acc,\n",
    "        \"Top-3 Accuracy\": top3_acc,\n",
    "        \"Top-10 Accuracy\": top10_acc\n",
    "    }\n",
    "\n",
    "results = {}\n",
    "for model_name, model in models.items():\n",
    "    results[model_name] = evaluate_model(model, X_test, y_test)\n",
    "\n",
    "# Display results\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"\\n{model_name} Model:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model performance comparison\n",
    "metrics = list(results[list(results.keys())[0]].keys())\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for i, (model_name, model_results) in enumerate(results.items()):\n",
    "    values = list(model_results.values())\n",
    "    ax.bar(x + i * width, values, width, label=model_name)\n",
    "\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Model Performance Comparison')\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(metrics, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
